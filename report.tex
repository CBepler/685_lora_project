\documentclass[10pt,letterpaper]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[margin=0.75in]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{cite}
\usepackage{multirow}
\usepackage{array}
\usepackage{caption}

% Formatting
\setlength{\parskip}{0.3em}
\captionsetup{font=small}

% Title and Author Information
\title{\textbf{Exploring Sparsity in LLMs via LoRA:\\A Comparative Study of Parameter-Efficient Fine-Tuning}}

\author{
    ECE 685D - Fall 2025 $\cdot$ Duke University
}

\date{}

\begin{document}

\maketitle
\vspace{-1em}

\begin{abstract}
\small
We compare four parameter-efficient fine-tuning methods for large language models: LoRA, Sparse LoRA, QLoRA, and HiRA. Evaluated on IMDB, SST-2, and WikiText-2 using DistilBERT (BERT-base for QLoRA due to compatibility), our results show: (1) QLoRA achieves highest classification accuracy (93.28\% IMDB, 92.20\% SST-2) with 50\% memory reduction but 2.5-3.5$\times$ slower training; (2) HiRA best for language modeling (perplexity 2.62); (3) Sparse LoRA achieves 50\% sparsity with minimal accuracy loss; (4) Standard LoRA provides best speed-accuracy balance.
\end{abstract}

\section{Introduction}
\vspace{-0.5em}

Large Language Models require billions of parameters, making full fine-tuning computationally expensive. Parameter-efficient fine-tuning (PEFT) methods address this by updating only a small subset of parameters. We investigate four LoRA variants to understand accuracy-efficiency trade-offs: (1) \textbf{LoRA}: baseline low-rank adaptation; (2) \textbf{Sparse LoRA}: adds sparsity constraints; (3) \textbf{QLoRA}: 4-bit quantization; (4) \textbf{HiRA}: high-rank adaptation. Our evaluation provides practical guidance for method selection based on task type and deployment constraints.

\section{Literature Review}
\vspace{-0.5em}

\textbf{LoRA} \cite{hu2022lora}: Represents weight updates as $W = W_0 + BA$ where $B \in \mathbb{R}^{d \times r}$, $A \in \mathbb{R}^{r \times k}$, and $r \ll \min(d,k)$. Reduces trainable parameters from $d \times k$ to $r(d+k)$ with no inference latency.

\textbf{Sparse LoRA} \cite{khaki2025sparselora}: Applies magnitude-based pruning to LoRA matrices with L1 regularization: $\mathcal{L} = \mathcal{L}_{\text{task}} + \lambda \|BA\|_1$. Achieves 2-3$\times$ inference speedup at 50\% sparsity.

\textbf{QLoRA} \cite{dettmers2023qlora}: Uses 4-bit NormalFloat (NF4) quantization with double quantization and paged optimizers. Enables 4$\times$ memory reduction while maintaining accuracy.

\textbf{HiRA} \cite{huang2025hira}: Uses Hadamard product for high-rank adaptation: $\Delta W = (B_1 \odot B_2)(A_1 \odot A_2)$. Achieves effective rank of $\min(d, k, r^2)$ with only $2r(d+k)$ parameters.

\textbf{DistilBERT} \cite{sanh2019distilbert}: 66M parameter distilled BERT (6 layers) retaining 97\% performance while 60\% faster. Used as base model except QLoRA (BERT-base, 110M parameters).

\section{Methodology}
\vspace{-0.5em}

\subsection{Datasets}
\vspace{-0.5em}
\textbf{IMDB}: 25k train, 25k test movie reviews for binary sentiment classification.
\textbf{SST-2}: 67k train, 1.8k test sentences for sentiment analysis.
\textbf{WikiText-2}: 2M tokens for masked language modeling.

\subsection{Implementation}
\vspace{-0.5em}
\textbf{Base Models}: DistilBERT-base-uncased (LoRA, Sparse LoRA, HiRA) and BERT-base-uncased (QLoRA due to quantization library compatibility).

\textbf{Hyperparameters}: Rank $r=8$ ($r=32$ for HiRA), $\alpha=16$ (64 for HiRA), target modules: q\_lin and v\_lin, learning rate 3e-4, batch size 8, 3 epochs, AdamW optimizer.

\textbf{Sparse LoRA}: 50\% target sparsity, magnitude-based pruning, L1 regularization $\lambda=0.01$.

\textbf{QLoRA}: 4-bit NF4 quantization, double quantization enabled, BFloat16 compute, 8-bit paged AdamW.

\subsection{Metrics}
\vspace{-0.5em}
\textbf{Performance}: Accuracy, F1-score (classification); Loss, Perplexity (language modeling).
\textbf{Efficiency}: Trainable parameters, training time, GPU memory, model size, inference throughput.

\section{Results}
\vspace{-0.5em}

\subsection{Classification Performance}
\vspace{-0.5em}

\begin{table}[h]
\centering
\small
\caption{IMDB Sentiment Classification Results}
\vspace{-0.5em}
\begin{tabular}{@{}lccccc@{}}
\toprule
\textbf{Method} & \textbf{Acc.} & \textbf{F1} & \textbf{Train \%} & \textbf{Time} & \textbf{Memory} \\
\midrule
LoRA & 92.52\% & 0.925 & 1.09\% & 35.2 min & 258 MB \\
Sparse LoRA & 92.48\% & 0.925 & 1.09\% & 35.2 min & 258 MB \\
QLoRA & \textbf{93.28\%} & \textbf{0.933} & \textbf{0.40\%} & 100.8 min & \textbf{134 MB} \\
HiRA & 92.93\% & 0.929 & 1.75\% & 35.4 min & 258 MB \\
\bottomrule
\end{tabular}
\label{tab:imdb}
\end{table}

\begin{table}[h]
\centering
\small
\caption{SST-2 Sentiment Classification Results}
\vspace{-0.5em}
\begin{tabular}{@{}lccccc@{}}
\toprule
\textbf{Method} & \textbf{Acc.} & \textbf{F1} & \textbf{Train \%} & \textbf{Time} & \textbf{Memory} \\
\midrule
LoRA & 90.02\% & 0.900 & 1.09\% & 13.6 min & 258 MB \\
Sparse LoRA & 89.22\% & 0.892 & 1.09\% & 13.6 min & 258 MB \\
QLoRA & \textbf{92.20\%} & \textbf{0.922} & \textbf{0.40\%} & 46.9 min & \textbf{134 MB} \\
HiRA & 90.14\% & 0.901 & 1.75\% & 13.6 min & 258 MB \\
\bottomrule
\end{tabular}
\label{tab:sst2}
\end{table}

QLoRA achieves highest accuracy on both tasks (+0.76\% IMDB, +2.18\% SST-2) with 50\% memory reduction, but requires \textbf{2.9-3.5$\times$ longer training}. Sparse LoRA maintains accuracy within 0.04-0.80\% of LoRA.

\subsection{Language Modeling Performance}
\vspace{-0.5em}

\begin{table}[h]
\centering
\small
\caption{WikiText-2 Language Modeling Results}
\vspace{-0.5em}
\begin{tabular}{@{}lccccc@{}}
\toprule
\textbf{Method} & \textbf{Loss} & \textbf{Perplexity} & \textbf{Train \%} & \textbf{Time} & \textbf{Throughput} \\
\midrule
LoRA & 0.987 & 2.68 & 0.22\% & 24.1 min & 181.7 s/s \\
Sparse LoRA & 0.994 & 2.70 & 0.22\% & 24.1 min & ~180 s/s \\
QLoRA & 1.163 & 3.20 & 0.40\% & 60.4 min & 100.1 s/s \\
HiRA & \textbf{0.964} & \textbf{2.62} & 0.87\% & 24.2 min & 169.9 s/s \\
\bottomrule
\end{tabular}
\label{tab:wikitext}
\end{table}

HiRA achieves lowest perplexity (2.62), demonstrating that high-rank benefits language modeling. QLoRA performs \textbf{worst} (perplexity 3.20, +19\% worse than LoRA), revealing task-dependent quantization effects.

\subsection{Efficiency Analysis}
\vspace{-0.5em}

\textbf{Parameter Efficiency}: QLoRA uses only 443k trainable parameters (0.40\%) vs. 740k for LoRA (1.09\%) and 1.18M for HiRA (1.75\%).

\textbf{Memory}: QLoRA reduces model size from 256-258 MB to 134-136 MB (48\% reduction). Peak GPU memory: 2.26 GB (LoRA), <2 GB (QLoRA).

\textbf{Inference Speed}: LoRA achieves 128-182 samples/sec, QLoRA 45-100 samples/sec, HiRA ~120-170 samples/sec. Quantization overhead reduces QLoRA throughput.

\textbf{Sparsity}: Sparse LoRA achieves 50\% sparsity with minimal accuracy impact (-0.04\% IMDB, -0.80\% SST-2, +0.02 perplexity).

\section{Discussion}
\vspace{-0.5em}

\subsection{Key Findings}
\vspace{-0.5em}

\textbf{Task-Dependent Performance}: Quantization shows opposite effects across tasks. QLoRA improves classification accuracy (+0.76-2.18\%) but degrades language modeling (+19\% perplexity). This suggests: (1) classification is robust to quantization noise, possibly with regularization benefits; (2) generative tasks require higher precision for accurate next-token prediction.

\textbf{Speed-Accuracy Trade-off}: QLoRA's superior classification accuracy and memory efficiency comes at 2.5-3.5$\times$ training time cost. Decision depends on: single training run with memory constraints (choose QLoRA) vs. rapid iteration/hyperparameter tuning (choose LoRA/HiRA).

\textbf{High-Rank Benefits}: HiRA's rank-32 adaptation achieves best language modeling (perplexity 2.62 vs. 2.68 for LoRA), validating that complex generative tasks benefit from increased capacity.

\textbf{"Free" Sparsity}: Sparse LoRA achieves 50\% sparsity with no training speed penalty (35.22 vs. 35.21 min) and minimal accuracy loss. With specialized sparse operations, could enable 2-3$\times$ inference speedup.

\subsection{Practical Recommendations}
\vspace{-0.5em}

\begin{itemize}
    \item \textbf{Limited GPU Memory + Classification}: QLoRA (accept 3$\times$ slower training for 50\% memory savings and best accuracy)
    \item \textbf{Fast Development/Iteration}: Standard LoRA (3$\times$ faster than QLoRA, reliable across all tasks)
    \item \textbf{Production Deployment}: Sparse LoRA (50\% sparsity enables faster inference with minimal accuracy loss)
    \item \textbf{Language Modeling/Generation}: HiRA (best perplexity 2.62; avoid QLoRA)
    \item \textbf{General Purpose}: Standard LoRA (best all-around performance)
\end{itemize}

\subsection{Limitations}
\vspace{-0.5em}

QLoRA requires BERT-base (vs. DistilBERT) due to quantization library constraints, preventing direct parameter comparison. Evaluation limited to sentiment classification and language modeling; other tasks (QA, NER, summarization) may show different patterns. Sparse LoRA benefits require specialized libraries for inference optimization.

\section{Conclusion}
\vspace{-0.5em}

This study evaluates four parameter-efficient fine-tuning methods across classification and language modeling tasks. Key contributions: (1) QLoRA excels at classification (93.28\% accuracy, 50\% memory reduction) but struggles with generation (worst perplexity 3.20) and requires 2.5-3.5$\times$ longer training; (2) HiRA optimal for language modeling (perplexity 2.62); (3) Sparse LoRA provides "free" 50\% sparsity with minimal accuracy impact; (4) Standard LoRA offers best speed-accuracy balance. These findings provide actionable guidance: method selection depends on task type (classification vs. generation), hardware constraints (memory, compute), and deployment requirements (training vs. inference speed).

\textbf{Future Work}: Hybrid methods combining QLoRA quantization with Sparse LoRA sparsity; adaptive per-layer rank selection; evaluation on larger models (7B-70B parameters); broader task diversity (QA, summarization); investigation of 6-bit and 8-bit quantization; specialized sparse operations for Sparse LoRA inference optimization.

\vspace{0.5em}
\noindent\textbf{Acknowledgments}: We thank course instructors and TAs (Haoming, Zihao) for guidance, and the HuggingFace team for Transformers, PEFT, and Datasets libraries.

\begin{thebibliography}{9}
\small

\bibitem{hu2022lora}
Hu, E.J., et al.
\textit{LoRA: Low-Rank Adaptation of Large Language Models.}
ICLR 2022.

\bibitem{khaki2025sparselora}
Khaki, S., et al.
\textit{SparseLoRA: Accelerating LLM Fine-Tuning with Contextual Sparsity.}
arXiv:2506.16500, 2025.

\bibitem{dettmers2023qlora}
Dettmers, T., et al.
\textit{QLoRA: Efficient Finetuning of Quantized LLMs.}
arXiv:2305.14314, 2023.

\bibitem{huang2025hira}
Huang, Q., et al.
\textit{HiRA: Parameter-Efficient Hadamard High-Rank Adaptation.}
ICLR 2025.

\bibitem{sanh2019distilbert}
Sanh, V., et al.
\textit{DistilBERT: Smaller, Faster, Cheaper and Lighter.}
arXiv:1910.01108, 2019.

\end{thebibliography}

\end{document}
