# Configuration file for LoRA Experiments
# ECE 685D - Fall 2025

# Model Configuration
model:
  name: "distilbert-base-uncased"  # Base model for all experiments
  cache_dir: "~/.cache/huggingface"

# Dataset Configuration
datasets:
  sst2:
    name: "stanfordnlp/sst2"
    text_column: "sentence"
    label_column: "label"
    num_labels: 2
    task_type: "classification"
    max_length: 128

  imdb:
    name: "stanfordnlp/imdb"
    text_column: "text"
    label_column: "label"
    num_labels: 2
    task_type: "classification"
    max_length: 512

  wikitext2:
    name: "Salesforce/wikitext"
    config: "wikitext-2-raw-v1"
    text_column: "text"
    task_type: "generation"
    max_length: 256

# Training Configuration (shared across all methods)
training:
  num_epochs: 3
  batch_size: 16
  gradient_accumulation_steps: 1
  learning_rate: 3.0e-4
  weight_decay: 0.01
  warmup_steps: 100
  max_grad_norm: 1.0
  eval_steps: 500
  save_steps: 500
  logging_steps: 100
  seed: 42
  fp16: true  # Use mixed precision training
  dataloader_num_workers: 4

# LoRA Configuration (Baseline)
lora:
  r: 8  # Rank of low-rank matrices
  lora_alpha: 16  # Scaling factor
  lora_dropout: 0.1
  target_modules:  # Modules to apply LoRA to
    - "q_lin"
    - "v_lin"
  bias: "none"
  task_type: "SEQ_CLS"  # or "CAUSAL_LM" for generation
  inference_mode: false

# Sparse LoRA Configuration
sparse_lora:
  r: 8
  lora_alpha: 16
  lora_dropout: 0.1
  target_modules:
    - "q_lin"
    - "v_lin"
  bias: "none"
  task_type: "SEQ_CLS"

  # Sparsity-specific parameters
  sparsity_method: "magnitude"  # Options: "magnitude", "l1", "topk"
  sparsity_ratio: 0.5  # Target 50% sparsity
  pruning_schedule: "gradual"  # Options: "oneshot", "gradual"
  pruning_freq: 100  # Steps between pruning operations
  l1_lambda: 0.001  # L1 regularization coefficient

# QLoRA Configuration
qlora:
  r: 8
  lora_alpha: 16
  lora_dropout: 0.1
  target_modules:
    - "q_lin"
    - "v_lin"
  bias: "none"
  task_type: "SEQ_CLS"

  # Quantization-specific parameters
  load_in_4bit: true
  bnb_4bit_compute_dtype: "float16"
  bnb_4bit_quant_type: "nf4"  # NormalFloat 4-bit
  bnb_4bit_use_double_quant: true  # Double quantization
  bnb_4bit_quant_storage: "uint8"

# HiRA Configuration
hira:
  r: 32  # Higher rank than standard LoRA
  lora_alpha: 64
  lora_dropout: 0.1
  target_modules:
    - "q_lin"
    - "v_lin"
  bias: "none"
  task_type: "SEQ_CLS"

  # HiRA-specific parameters
  use_hadamard: true  # Enable Hadamard product
  hadamard_dim: 32

# Evaluation Configuration
evaluation:
  batch_size: 32
  metrics:
    classification:
      - "accuracy"
      - "f1"
      - "precision"
      - "recall"
    generation:
      - "perplexity"
      - "bleu"

  # Performance metrics to track
  track_metrics:
    - "inference_time"
    - "memory_usage"
    - "model_size"
    - "trainable_parameters"
    - "total_parameters"
    - "training_time"
    - "gpu_memory_peak"

# Experiment Tracking
experiment:
  use_wandb: false  # Set to true to enable Weights & Biases logging
  project_name: "ece685-lora-comparison"
  entity: null  # Set your wandb entity/username
  save_dir: "results"

# Hardware Configuration
hardware:
  device: "cuda"  # "cuda" or "cpu"
  mixed_precision: "fp16"  # "fp16", "bf16", or "no"

# Reproducibility
reproducibility:
  seed: 42
  deterministic: true
  benchmark: false
